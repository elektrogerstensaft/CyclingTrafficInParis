import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import geopandas as gpd

# in rare cases there might be a SSL error:
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# import the main dataframe
df = pd.read_csv("https://fwpn.uber.space/media/CyclingTrafficInParis_eng.csv")

#st.set_page_config(layout="wide")   #this eliminates margins left and right on wider screens, but some plots do not work well with it 

st.image("Header.png", caption="Image generated by Midjourney AI")
st.title("A DATA ANALYSIS OF THE CYCLING TRAFFIC IN PARIS")
st.header("From October 2022 to November 2023")

## table of contents
st.sidebar.title("Table of contents")
pages=["Summary", "Cycling Traffic", "Weather & Traffic", "Interview / Barometer", "Machine Learning"]
page=st.sidebar.radio("Go to", pages)

## about
st.sidebar.markdown("---")

st.sidebar.markdown(
  """
  <div style="background-color: #285562; border: 1px solid #85d2db; padding: 10px; border-radius: 5px;">

  **Authors:**
  - Martin Kruse
  - Andy Soydt
  - Marine Bajard-Malfondet

  **Data sources:**
  - [Comptage vélo | Open Data | Ville de Paris](https://opendata.paris.fr/explore/dataset/comptage-velo-donnees-compteurs/information/?disjunctive.id_compteur&disjunctive.nom_compteur&disjunctive.id&disjunctive.name) 
  - [Observation météorologique historiques France | OpenDataSoft](https://public.opendatasoft.com/explore/dataset/donnees-synop-essentielles-omm/export/?q=paris&refine.nom=ORLY&q.timerange.date=date:%5B2022-09-30T22:00:00Z+TO+2023-10-31T22:59:59Z%5D&sort=date&dataChart=eyJxdWVyaWVzIjpbeyJjaGFydHMiOlt7InR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQVZHIiwieUF4aXMiOiJ0YyIsInNjaWVudGlmaWNEaXNwbGF5Ijp0cnVlLCJjb2xvciI6IiNGRjUxNUEifV0sInhBeGlzIjoiZGF0ZSIsIm1heHBvaW50cyI6IiIsInRpbWVzY2FsZSI6ImRheSIsInNvcnQiOiIiLCJjb25maWciOnsiZGF0YXNldCI6ImRvbm5lZXMtc3lub3AtZXNzZW50aWVsbGVzLW9tbSIsIm9wdGlvbnMiOnsicSI6InBhcmlzIiwicmVmaW5lLm5vbSI6Ik9STFkiLCJxLnRpbWVyYW5nZS5kYXRlIjoiZGF0ZTpbMjAyMi0wOS0zMFQyMjowMDowMFogVE8gMjAyMy0xMC0zMVQyMjo1OTo1OVpdIiwic29ydCI6ImRhdGUifX19XSwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D)
  - [Baromètres Parlons vélo | OpenData](https://barometre.parlons-velo.fr/)

  </div>
  """,
  unsafe_allow_html=True
)

## pages /w content

if page == pages[0]:
  st.title("Summary")
  st.markdown(
    """
    ### Introduction

    This data analysis conducts a comprehensive examination of cycling traffic within the city of Paris, utilizing publicly available data on cycling counts, meteorological conditions, and national holidays. The study yields insightful observations and valuable outcomes.

    ### Data Gathering and Processing

    In gathering and processing the data, a variety of factors were considered, going beyond the surface to understand the dynamics of traffic. This section delves into a careful analysis of counts and explores their interplay with changing weather patterns. The outcomes are presented through vivid traffic overview maps and a variety of charts, creating a clear and logical picture of the cycling scenario in Paris.

    ### Predictive Modeling with Machine Learning

    Leveraging the capabilities of machine learning, we designed predictive models that successfully anticipate cycling traffic patterns. This section details our approach, sharing insights gained from our models. The predictions provide a glimpse into the potential future cycling trends in the city.

    ### Executive Summary

    This executive summary aims to provide a brief yet precise overview of our comprehensive data analysis. It serves as a starting point for a deeper exploration within the complete report.
    """
  )

if page == pages[1]:
    st.title("Cycling Traffic")
  
    df["Date and time of count"] = pd.to_datetime(df["Date and time of count"], utc= True)

    weekdays = {
        0: "Monday", 1: "Tuesday", 2: "Wednesday", 3: "Thursday", 4: "Friday", 5: "Saturday", 6: "Sunday"}
    df["weekday_of_count"] = df["weekday_of_count"].map(weekdays)

    monthReduceDict = {
        "2022-10": "Oct / Nov 22",
        "2022-11": "Oct / Nov 22",
        "2022-12": "Dec 22 / Jan 23",
        "2023-01": "Dec 22 / Jan 23",
        "2023-02": "Feb / Mar 23",
        "2023-03": "Feb / Mar 23",
        "2023-04": "Apr / May 23",
        "2023-05": "Apr / May 23",
        "2023-06": "Jun / Jul 23",
        "2023-07": "Jun / Jul 23",
        "2023-08": "Aug / Sep 23",
        "2023-09": "Aug / Sep 23",
        "2023-10": "Oct / Nov 23",
        "2023-11": "Oct / Nov 23"
        }
    df["Months reduced"] = df["Month and year of count"].map(monthReduceDict)

    # finding the top 3 counters and creating a dataframe only with them
    df_top3 = df.groupby(["Counter name"],as_index= False)["Hourly count"].sum().sort_values("Hourly count", ascending = False).head(3)

    top3 = []
    for x in df_top3["Counter name"]:
        top3.append(x)
    df_top3 = df.loc[df["Counter name"].isin(top3)]


    st.write("### Cycling Traffic")
    st.write("#### Initial Data")
    # presentation of the data (volume, architecture, etc.)
    # data analysis using DataVizualization figures
    st.write("The initial data are the hourly counts of bicycles at different counting sites in Paris from October 2022 to November 2023. \
            For every hour and every counter a line was added to the dataset. The target variable is *Hourly Count*. As metadata was added to every line, \
            the file is bloated with repetetive information like URL of photographs or installation dates. Irrelevant metadata was removed. \
            Each counting site can have two *counters* - one for each direction of a street. The direction is encoded in the counter name and was translated into a separate column \
            The *date and time* information is stored in a single variable and was processed into: date, time, ISO week and year, day (of month), day of week. Month of year was already present \
            The *geographic coordinates* were present in a combined column and were separated into latitude and longitude.            ")
    st.write("#### Data Cleaning")
    st.write("Some entries appeared to be older than the timeframe and were removed. Counts with 0 or more than 2000 bicycles were also removed.")

    fig = px.box(df_top3, y ="Hourly count", x = "Months reduced", title = "All counters hourly counts")
    fig.update_layout(font=dict(size=20))
    st.plotly_chart(fig)

    st.write("The time domain shows a distribution related to working days vs. weekends and to the hour of day. Daily commutes appear very well in the heatmap.")
    # Heatmap of days and hours with most traffic
    order = ["Sunday", "Saturday", "Friday", "Thursday", "Wednesday", "Tuesday","Monday"]

    grouped_multiple = df_top3.groupby(["hour_of_day", 'weekday_of_count']).agg({'Hourly count': ["mean", "median","sum"]})
    grouped_multiple.columns = ["Hourly_count_mean", "Hourly_count_median","Hourly_count_sum"]
    grouped_multiple = grouped_multiple.reset_index()

    fig = go.Figure(
        data = go.Heatmap(
            z = grouped_multiple["Hourly_count_sum"],
            x = grouped_multiple["hour_of_day"],
            y = grouped_multiple["weekday_of_count"]
        )
    )
    fig.update_xaxes(title = "Hour of day")
    fig.update_yaxes(title = "Weekday", categoryarray = order)
    fig.update_layout(
        title="Heatmap of daytimes per weekday with most bicycle traffic",
        font=dict(size=20))
    st.plotly_chart(fig)

    st.write("As the heatmap shows, traffic is highest in the moring and has a second peak in the afternoon on working days. A third peak can be observed in the evening, at 9pm. \
             We assume the first peak is caused by commuters riding to work and the second peak consists of them riding back. It remains unclear why the first peak is 30-40% smaller \
             than the second. As the top 2 counters are oriented in opposite directions of the same counting site, the simple assumption of people going in one direction \
             in the morning and returning in the afternoon does not line up with the different sized peaks. Either cyclists avoid the counter sites in mornings, \
             or a part of the cyclists who travel in one direction midday also travel back in the afternoon.")
    fig = px.line(df.loc[(df["Counter name"].isin(top3)) & (df["week_year"] == "2023-23")].sort_values("Date and time of count"),
    x="Date and time of count",
    y="Hourly count",
    color = "Counter name")
    fig.update_layout(
        title ="Hourly count of bicycles at top 3 counters",
        legend=dict(yanchor="top", y=0.99, xanchor="right", x=0.99),
        font = dict(size=20))
    st.plotly_chart(fig)

    st.write("#### Vacation Dates")
    st.write("As weekends have less traffic, it was estimated, that other vacation days (e.g. christmas or summer holidays) also influence the traffic. ")
    df_w = pd.read_csv("WeatherAndTraffic.csv", sep = ",")

    grouped_multiple = df_top3.groupby(["date","holiday"]).agg({"Hourly count": "sum"})
    grouped_multiple = grouped_multiple.reset_index()

    g = sns.lineplot(data=grouped_multiple, x = "date", y= "Hourly count")

    #sns.scatterplot(data=grouped_multiple, x = "date", y= "Hourly count")
    g2 = sns.scatterplot(data=grouped_multiple, x = "date", y= "holiday", ax = g.axes.twinx())
    g.set(xlabel = "Date", ylabel = "Daily count", title ="Number of bicycles per day and holiday")
    sns.set(font_scale=1.25)
    new_ticks = [i.get_text() for i in g.get_xticklabels()]
    plt.xticks(range(0, len(new_ticks), 30), new_ticks[::30])
    g.set_xticklabels(g.get_xticklabels(), rotation=45)
    st.pyplot(g.get_figure())

    st.write("The counting sites are spread heterogeneous over the city: \n      \
              the North-South axis (Gare du Nord/ Gare de l’Est – Châtelet – Odéon) and the Seine banks are best covered \n \
              the north of Paris and the ring (in particular West/ North) have fewer counter sites \n \
              some arrondissements not having in addition any counter site at all \n \
              The dot size on the map indicates how many bicycles were counted. It appears that traffic is higher in the centre. The three counters with the most traffic are:" \
              , df_top3["Counter name"].unique())
    #st.write(df_top3["Counter name"].unique())
    df_counter= pd.read_csv("Counters.csv", sep= ",")
    df_counter.set_index("Counter name", inplace = True)
    df_reduced = df.drop(["Counter ID","Counting site installation date","Geographic coordinates", "Counting site ID"], axis = 1)   
    df_reduced.rename({"Hourly count":"Total count"}, axis="columns", inplace=True)

    #grouping by the Counter name, aggregation by sum of hourly counts 
    df_reduced = df_reduced.groupby(["Counter name"],as_index= True)["Total count"].sum()

    # merge the previous df with the Counter metadata df
    df_geo = pd.concat([df_reduced, df_counter], axis=1)
    df_geo.dropna(inplace=True)
    df_geo.set_index("Counter ID", inplace = True)

    # generating a GeoDataFrame 
    gdf = gpd.GeoDataFrame(
        df_geo, geometry=gpd.points_from_xy(df_geo.Longitude, df_geo.Latitude), crs="EPSG:4326"
    )

    # creating a scatter plot on a map background
    fig = px.scatter_mapbox(gdf,
                            lat=gdf.geometry.y,
                            lon=gdf.geometry.x,
                            size = "Total count",
                            color = "Total count",
                            hover_name="Counting site name",
                            hover_data="Total count",
                            color_continuous_scale=px.colors.sequential.Viridis,
                            zoom=10.9,
                            title = "Total counted bicycles in Paris")
    fig.update_layout(mapbox_style="carto-positron",
                    margin={"r":0,"t":0,"l":0,"b":0},
                    font=dict(size=18, color="Black"))
    st.plotly_chart(fig)

if page == pages[2] : 
  st.title("Weather & Traffic")




if page == pages[3] : 
  st.title("Interview & Barometer")

if page == pages[4] : 
  st.title("Machine Learning")
  st.write("### Data Preprocessing")
  st.write("#### Train, Test, Split")
  st.write("Before applying any normalisation or encoding, the complete data frame was divided into feature variables and the target variable *hourly counts*. \
           With train_test_split from the scikit learn library the inputs were divided into  a train set and a test set.")
  code = """feats = df[["hour_of_day", "weekday_of_count", "Month and year of count", "day", "Latitude", "Longitude", "Humidity", "Temp_°C", "Rain_last3H", "direction","holiday"]]
target = df["Hourly count"]

X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.25, random_state=42)
  """
  st.code(code, language='python')
  st.write("The feature were split into 3 variable types: categorical, numerical and cyclical.")
  code="""cat = ["direction", "Month and year of count","holiday"]
    num = ["day", "Latitude", "Longitude","Humidity","Temp_°C","Rain_last3H"]
    circular = ["hour_of_day", "weekday_of_count"]"""
  st.code(code, language= "python")

  st.write("#### Normalisation")  
  st.write("The numerical variables were treated with the scikit learn standard scaler method.")
  code="""from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    X_train[num] = sc.fit_transform(X_train[num])
    X_test[num] = sc.transform(X_test[num])"""
  st.code(code, language= "python")

  st.write("#### Encoding")
  st.write("The categorical variables were treated with the scikit learn one hot encoder method.")
  code="""from sklearn.preprocessing import OneHotEncoder
    ohe = OneHotEncoder(drop="first",  sparse_output=False)
    X_train_Cat = pd.DataFrame(ohe.fit_transform(X_train[cat]))
    X_train_Cat.columns= ohe.get_feature_names_out()"""
  st.code(code, language= "python")

  st.write("The cyclical variables were treated with sine and cosine functions like this:")
  code="""circular_train.loc[:, 'sin_hour'] = circular_train.loc[:, 'hour_of_day'].apply(lambda h : np.sin(2 * np.pi * h / 24))
    circular_train.loc[:, 'cos_hour'] = circular_train.loc[:, 'hour_of_day'].apply(lambda h : np.cos(2 * np.pi * h / 24))"""
  st.code(code, language= "python")
 


  st.write("### Algorithms")
  st.write("Since the target variable is numerical and not binary, the prediction is a regression problem. The following regression algorithms were tested:")
  lst = ["Linear Regression","Decision Tree Regression","K-Nearest-Neighbor (KNN)","Multilayer perceptron (MLP)","Random Forest Regression","Gradient Boosting Regression"]
  for i in lst:
    st.markdown("- " + i)
  st.write("The scores of the algorithms were used to determine which algorithms to look further into.")
  
  st.write("### Predictions")