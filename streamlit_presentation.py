import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import geopandas as gpd

# in rare cases there might be a SSL error:
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# import the main dataframe
df = pd.read_csv("https://drive.google.com/file/d/1uleq1dLKq-2YW6xPpaiuiQBAVwV8Xvo0/view?usp=sharing")

#st.set_page_config(layout="wide")   #this eliminates margins left and right on wider screens, but some plots do not work well with it 

st.image("Header.png", caption="Image generated by Midjourney AI")
st.title("A DATA ANALYSIS OF THE CYCLING TRAFFIC IN PARIS")
st.header("From October 2022 to November 2023")

## table of contents
st.sidebar.title("Table of contents")
pages=["Summary", "Cycling Traffic", "Weather & Traffic", "Interview / Barometer", "Machine Learning"]
page=st.sidebar.radio("Go to", pages)

## about
st.sidebar.markdown("---")

st.sidebar.markdown(
  """
  <div style="background-color: #285562; border: 1px solid #85d2db; padding: 10px; border-radius: 5px;">

  **Authors:**
  - Martin Kruse
  - Andy Soydt
  - Marine Bajard-Malfondet

  **Data sources:**
  - [Comptage vélo | Open Data | Ville de Paris](https://opendata.paris.fr/explore/dataset/comptage-velo-donnees-compteurs/information/?disjunctive.id_compteur&disjunctive.nom_compteur&disjunctive.id&disjunctive.name) 
  - [Observation météorologique historiques France | OpenDataSoft](https://public.opendatasoft.com/explore/dataset/donnees-synop-essentielles-omm/export/?q=paris&refine.nom=ORLY&q.timerange.date=date:%5B2022-09-30T22:00:00Z+TO+2023-10-31T22:59:59Z%5D&sort=date&dataChart=eyJxdWVyaWVzIjpbeyJjaGFydHMiOlt7InR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQVZHIiwieUF4aXMiOiJ0YyIsInNjaWVudGlmaWNEaXNwbGF5Ijp0cnVlLCJjb2xvciI6IiNGRjUxNUEifV0sInhBeGlzIjoiZGF0ZSIsIm1heHBvaW50cyI6IiIsInRpbWVzY2FsZSI6ImRheSIsInNvcnQiOiIiLCJjb25maWciOnsiZGF0YXNldCI6ImRvbm5lZXMtc3lub3AtZXNzZW50aWVsbGVzLW9tbSIsIm9wdGlvbnMiOnsicSI6InBhcmlzIiwicmVmaW5lLm5vbSI6Ik9STFkiLCJxLnRpbWVyYW5nZS5kYXRlIjoiZGF0ZTpbMjAyMi0wOS0zMFQyMjowMDowMFogVE8gMjAyMy0xMC0zMVQyMjo1OTo1OVpdIiwic29ydCI6ImRhdGUifX19XSwiZGlzcGxheUxlZ2VuZCI6dHJ1ZSwiYWxpZ25Nb250aCI6dHJ1ZX0%3D)
  - [Baromètres Parlons vélo | OpenData](https://barometre.parlons-velo.fr/)

  </div>
  """,
  unsafe_allow_html=True
)

## pages /w content

if page == pages[0]:
  st.title("Summary")
  st.markdown(
    """
    ### Introduction

    This data analysis conducts a comprehensive examination of cycling traffic within the city of Paris, utilizing publicly available data on cycling counts, meteorological conditions, and national holidays. The study yields insightful observations and valuable outcomes.

    ### Data Gathering and Processing

    In gathering and processing the data, a variety of factors were considered, going beyond the surface to understand the dynamics of traffic. This section delves into a careful analysis of counts and explores their interplay with changing weather patterns. The outcomes are presented through vivid traffic overview maps and a variety of charts, creating a clear and logical picture of the cycling scenario in Paris.

    ### Predictive Modeling with Machine Learning

    Leveraging the capabilities of machine learning, we designed predictive models that successfully anticipate cycling traffic patterns. This section details our approach, sharing insights gained from our models. The predictions provide a glimpse into the potential future cycling trends in the city.

    ### Executive Summary

    This executive summary aims to provide a brief yet precise overview of our comprehensive data analysis. It serves as a starting point for a deeper exploration within the complete report.
    """
  )

if page == pages[1]:
    st.title("Cycling Traffic")
    df = pd.read_csv("CyclingTrafficInParis_eng.csv")
    df["Date and time of count"] = pd.to_datetime(df["Date and time of count"], utc= True)

    weekdays = {
        0: "Monday", 1: "Tuesday", 2: "Wednesday", 3: "Thursday", 4: "Friday", 5: "Saturday", 6: "Sunday"}
    df["weekday_of_count"] = df["weekday_of_count"].map(weekdays)

    monthReduceDict = {
        "2022-10": "Oct / Nov 22",
        "2022-11": "Oct / Nov 22",
        "2022-12": "Dec 22 / Jan 23",
        "2023-01": "Dec 22 / Jan 23",
        "2023-02": "Feb / Mar 23",
        "2023-03": "Feb / Mar 23",
        "2023-04": "Apr / May 23",
        "2023-05": "Apr / May 23",
        "2023-06": "Jun / Jul 23",
        "2023-07": "Jun / Jul 23",
        "2023-08": "Aug / Sep 23",
        "2023-09": "Aug / Sep 23",
        "2023-10": "Oct / Nov 23",
        "2023-11": "Oct / Nov 23"
        }
  df["Months reduced"] = df["Month and year of count"].map(monthReduceDict)

  # finding the top 3 counters and creating a dataframe only with them
  df_top3 = df.groupby(["Counter name"],as_index= False)["Hourly count"].sum().sort_values("Hourly count", ascending = False).head(3)

  top3 = []
  for x in df_top3["Counter name"]:
      top3.append(x)
  df_top3 = df.loc[df["Counter name"].isin(top3)]


  st.write("### Cycling Traffic")
  st.write("#### Initial Data")
  # presentation of the data (volume, architecture, etc.)
  # data analysis using DataVizualization figures
  st.write("The initial data are the hourly counts of bicycles at different counting sites in Paris from October 2022 to November 2023. \
           For every hour and every counter a line was added to the dataset. The target variable is *Hourly Count*. As metadata was added to every line, \
           the file is bloated with repetetive information like URL of photographs or installation dates. Irrelevant metadata was removed. \
           Each counting site can have two *counters* - one for each direction of a street. The direction is encoded in the counter name and was translated into a separate column \
           The *date and time* information is stored in a single variable and was processed into: date, time, ISO week and year, day (of month), day of week. Month of year was already present \
           The *geographic coordinates* were present in a combined column and were separated into latitude and longitude.            ")
  st.write("#### Data Cleaning")
  st.write("Some entries appeared to be older than the timeframe and were removed. Counts with 0 or more than 2000 bicycles were also removed.")

  fig = px.box(df_top3, y ="Hourly count", x = "Months reduced", title = "All counters hourly counts")
  fig.update_layout(font=dict(size=20))
  st.plotly_chart(fig)

  st.write("The time domain shows a distribution related to working days vs. weekends and to the hour of day. Daily commutes appear very well in the heatmap.")
  # Heatmap of days and hours with most traffic
  order = ["Sunday", "Saturday", "Friday", "Thursday", "Wednesday", "Tuesday","Monday"]

  grouped_multiple = df_top3.groupby(["hour_of_day", 'weekday_of_count']).agg({'Hourly count': ["mean", "median","sum"]})
  grouped_multiple.columns = ["Hourly_count_mean", "Hourly_count_median","Hourly_count_sum"]
  grouped_multiple = grouped_multiple.reset_index()

  fig = go.Figure(
      data = go.Heatmap(
          z = grouped_multiple["Hourly_count_sum"],
          x = grouped_multiple["hour_of_day"],
          y = grouped_multiple["weekday_of_count"]
      )
  )
  fig.update_xaxes(title = "Hour of day")
  fig.update_yaxes(title = "Weekday", categoryarray = order)
  fig.update_layout(
      title="Heatmap of daytimes per weekday with most bicycle traffic",
      font=dict(size=20))
  st.plotly_chart(fig)

  st.write("#### Vacation Dates")
  st.write("As weekends have less traffic, it was estimated, that other vacation days (e.g. christmas or summer holidays) also influence the traffic. ")
  df_w = pd.read_csv("WeatherAndTraffic.csv", sep = ",")

  grouped_multiple = df_top3.groupby(["date","holiday"]).agg({"Hourly count": "sum"})
  grouped_multiple = grouped_multiple.reset_index()

  g = sns.lineplot(data=grouped_multiple, x = "date", y= "Hourly count")

  #sns.scatterplot(data=grouped_multiple, x = "date", y= "Hourly count")
  g2 = sns.scatterplot(data=grouped_multiple, x = "date", y= "holiday", ax = g.axes.twinx())
  g.set(xlabel = "Date", ylabel = "Daily count", title ="Number of bicycles per day and holiday")
  sns.set(font_scale=1.25)
  new_ticks = [i.get_text() for i in g.get_xticklabels()]
  plt.xticks(range(0, len(new_ticks), 30), new_ticks[::30])
  g.set_xticklabels(g.get_xticklabels(), rotation=45)
  st.pyplot(g.get_figure())

  st.write("The counting sites are spread heterogeneous over the city: \n      \
            the North-South axis (Gare du Nord/ Gare de l’Est – Châtelet – Odéon) and the Seine banks are best covered \n \
            the north of Paris and the ring (in particular West/ North) have fewer counter sites \n \
            some arrondissements not having in addition any counter site at all \n \
            The dot size on the map indicates how many bicycles were counted. It appears that traffic is higher in the centre. The three counters with the most traffic are:" \
            , df_top3["Counter name"].unique())
  #st.write(df_top3["Counter name"].unique())
  df_counter= pd.read_csv("Counters.csv", sep= ",")
  df_counter.set_index("Counter name", inplace = True)
  df_reduced = df.drop(["Counter ID","Counting site installation date","Geographic coordinates", "Counting site ID"], axis = 1)   
  df_reduced.rename({"Hourly count":"Total count"}, axis="columns", inplace=True)

  #grouping by the Counter name, aggregation by sum of hourly counts 
  df_reduced = df_reduced.groupby(["Counter name"],as_index= True)["Total count"].sum()

  # merge the previous df with the Counter metadata df
  df_geo = pd.concat([df_reduced, df_counter], axis=1)
  df_geo.dropna(inplace=True)
  df_geo.set_index("Counter ID", inplace = True)

  # generating a GeoDataFrame 
  gdf = gpd.GeoDataFrame(
      df_geo, geometry=gpd.points_from_xy(df_geo.Longitude, df_geo.Latitude), crs="EPSG:4326"
  )

  # creating a scatter plot on a map background
  fig = px.scatter_mapbox(gdf,
                          lat=gdf.geometry.y,
                          lon=gdf.geometry.x,
                          size = "Total count",
                          color = "Total count",
                          hover_name="Counting site name",
                          hover_data="Total count",
                          color_continuous_scale=px.colors.sequential.Viridis,
                          zoom=10.9,
                          title = "Total counted bicycles in Paris")
  fig.update_layout(mapbox_style="carto-positron",
                  margin={"r":0,"t":0,"l":0,"b":0},
                  font=dict(size=18, color="Black"))
  st.plotly_chart(fig)

if page == pages[2] : 
  st.title("Weather & Traffic")




if page == pages[3] : 
  st.title("Interview & Barometer")

if page == pages[4] : 
  st.title("Machine Learning")
    
